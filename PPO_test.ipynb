{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_test.ipynb",
      "provenance": [],
      "mount_file_id": "1qVKqNKS-auDuvFlHHxYosvOV5UHan-h3",
      "authorship_tag": "ABX9TyMA97Fr1RW2asIcJbO/4E+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saez1007/PPO-Tensorflow-2.0/blob/master/PPO_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVPwBE0VBhAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67565886-ca66-4357-e4e3-fde850a17c60"
      },
      "source": [
        "!git clone https://github.com/saez1007/PPO-Tensorflow-2.0.git\n",
        "!pip install -e \"PPO-Tensorflow-2.0\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PPO-Tensorflow-2.0'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 142 (delta 30), reused 127 (delta 20), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (142/142), 1.33 MiB | 9.96 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "Obtaining file:///content/PPO-Tensorflow-2.0\n",
            "Collecting mlagents_envs==0.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/52/875dc661eca79febd669957cf5e3df297b5b33504938256d11a8ef7ec6ba/mlagents_envs-0.10.0-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (0.17.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (2.2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (2.2.2)\n",
            "Collecting pep8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/3f/669429ce58de2c22d8d2c542752e137ec4b9885fff398d3eceb1a7f5acb4/pep8-1.7.1-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/35/dcf9a3393305bfc61854b764b5aeb79a72493e77991eead133c189d7868e/flake8-3.8.2-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.3MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/6e/36419ec1bd2208e157dff7fc3e565b185394c0dc4901e9e2f983cb1d4b7f/pylint-2.5.2-py3-none-any.whl (324kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 20.1MB/s \n",
            "\u001b[?25hCollecting pyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (3.2.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (7.1.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (1.1.2)\n",
            "Collecting flask-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting websockets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (5.4.8)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from rl-tf-2==0.5) (1.3.17)\n",
            "Requirement already satisfied: numpy<2.0,>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from mlagents_envs==0.10->rl-tf-2==0.5) (1.18.4)\n",
            "Requirement already satisfied: protobuf>=3.6 in /usr/local/lib/python3.6/dist-packages (from mlagents_envs==0.10->rl-tf-2==0.5) (3.10.0)\n",
            "Requirement already satisfied: Pillow>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from mlagents_envs==0.10->rl-tf-2==0.5) (7.0.0)\n",
            "Requirement already satisfied: grpcio>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from mlagents_envs==0.10->rl-tf-2==0.5) (1.29.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->rl-tf-2==0.5) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->rl-tf-2==0.5) (1.5.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (0.9.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (2.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->rl-tf-2==0.5) (1.12.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->rl-tf-2==0.5) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->rl-tf-2==0.5) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->rl-tf-2==0.5) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->rl-tf-2==0.5) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->rl-tf-2==0.5) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->rl-tf-2==0.5) (1.6.0.post3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->rl-tf-2==0.5) (47.1.1)\n",
            "Collecting pycodestyle<2.7.0,>=2.6.0a1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8->rl-tf-2==0.5) (1.6.0)\n",
            "Collecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Collecting pyflakes<2.3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/5b/fd01b0c696f2f9a6d2c839883b642493b431f28fa32b29abc465ef675473/pyflakes-2.2.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[?25hCollecting astroid<=2.5,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/c9/e9c2642dfb169590fb8bdb395f9329da042ee559c2ae7c1e612a3e5f40b4/astroid-2.4.1-py3-none-any.whl (214kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 19.1MB/s \n",
            "\u001b[?25hCollecting toml>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
            "Collecting isort<5,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml->rl-tf-2==0.5) (3.13)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->rl-tf-2==0.5) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->rl-tf-2==0.5) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->rl-tf-2==0.5) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->rl-tf-2==0.5) (1.2.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->rl-tf-2==0.5) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->rl-tf-2==0.5) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->rl-tf-2==0.5) (0.16.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->rl-tf-2==0.5) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->rl-tf-2==0.5) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->rl-tf-2==0.5) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->rl-tf-2==0.5) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->rl-tf-2==0.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->rl-tf-2==0.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->rl-tf-2==0.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->rl-tf-2==0.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->rl-tf-2==0.5) (3.1.0)\n",
            "Collecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->rl-tf-2==0.5) (1.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->rl-tf-2==0.5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->rl-tf-2==0.5) (3.1.0)\n",
            "Installing collected packages: mlagents-envs, pep8, pycodestyle, mccabe, pyflakes, flake8, lazy-object-proxy, typed-ast, astroid, toml, isort, pylint, pyaml, flask-cors, websockets, rl-tf-2\n",
            "  Running setup.py develop for rl-tf-2\n",
            "Successfully installed astroid-2.4.1 flake8-3.8.2 flask-cors-3.0.8 isort-4.3.21 lazy-object-proxy-1.4.3 mccabe-0.6.1 mlagents-envs-0.10.0 pep8-1.7.1 pyaml-20.4.0 pycodestyle-2.6.0 pyflakes-2.2.0 pylint-2.5.2 rl-tf-2 toml-0.10.1 typed-ast-1.4.1 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2MbyuKuDxj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kernel needs to be restarted before detecting the package\n",
        "import gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "from sklearn import preprocessing\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from rl_tf_2.algorithms.ppo import CategoricalModel, GaussianModel, Roller, PolicyCombinedLoss, network_builder, Params, mlp\n",
        "from rl_tf_2.environment import create_batched_env\n",
        "from rl_tf_2.utils import Logger, log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5UqCQnxFf42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/data/Kraken_BTCUSD_d.csv\", skiprows= 2, names = ['date', 'symbol', 'open', 'high', 'low', 'close', 'vol_btc', 'vol']).sort_index(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUgwa2kqBn9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BitcoinTradingEnv(gym.Env):\n",
        "    \"\"\"A Bitcoin trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['live', 'file', 'none']}\n",
        "    scaler = preprocessing.MinMaxScaler()\n",
        "    viewer = None\n",
        "\n",
        "    def __init__(self,\n",
        "                 df,\n",
        "                 lookback_window_size=0,\n",
        "                 commission=0.00075,\n",
        "                 initial_balance=10000,\n",
        "                 max_session = 300,\n",
        "                 serial=False\n",
        "                 ):\n",
        "        super(BitcoinTradingEnv, self).__init__()\n",
        "        self.env_info = EnvInfo\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "        self.initial_balance = 0\n",
        "        # self.commission = commission\n",
        "        self.serial = serial\n",
        "        self.max_session = max_session\n",
        "        # Actions of the format Buy 1/10, Sell 3/10, Hold, etc.\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        # Observes the OHCLV values\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(5\n",
        "                                                                  ,self.lookback_window_size + 1\n",
        "                                                                  ), dtype=np.float16)\n",
        "\n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.btc_held = 0\n",
        "        self._reset_session()\n",
        "        self.account_history = np.repeat([\n",
        "        [0],\n",
        "        [0],\n",
        "        [0],\n",
        "        [0],\n",
        "        [0]\n",
        "        ], self.lookback_window_size + 1, axis=1)\n",
        "        self.trades = []\n",
        "        return self._next_observation()[0], {}, {}, {}\n",
        "\n",
        "    def _reset_session(self):\n",
        "        self.current_step = np.random.randint(0, len(self.df) - self.max_session)\n",
        "        if self.serial:\n",
        "            self.steps_left = len(self.df) - self.lookback_window_size - 1\n",
        "            self.frame_start = self.lookback_window_size\n",
        "        else:\n",
        "            self.steps_left = np.random.randint(1, self.max_session)\n",
        "            self.frame_start = np.random.randint(\n",
        "                self.lookback_window_size, len(self.df) - self.steps_left)\n",
        "            self.active_df = self.df[self.frame_start -\n",
        "                self.lookback_window_size:self.frame_start + self.steps_left]\n",
        "\n",
        "    def _next_observation(self):\n",
        "        end = self.current_step + self.lookback_window_size + 1\n",
        "        obs = np.array([\n",
        "        self.df['open'].values[self.current_step:end],\n",
        "        self.df['high'].values[self.current_step:end],\n",
        "        self.df['low'].values[self.current_step:end],\n",
        "        self.df['close'].values[self.current_step:end],\n",
        "        self.df['vol'].values[self.current_step:end],\n",
        "        ])\n",
        "        new_price = self.df['close'].values[self.current_step:][0]\n",
        "        scaled_history = self.scaler.fit_transform(self.account_history)\n",
        "        # obs = np.append(obs, scaled_history[:, -(self.lookback_window_size\n",
        "        #                                                  + 1):], axis=0)\n",
        "        return obs, new_price\n",
        "\n",
        "    def step(self, action):\n",
        "        current_price = self._get_current_price()\n",
        "        self._take_action(action, current_price)\n",
        "        self.steps_left -= 1\n",
        "        self.current_step += 1\n",
        "        if self.steps_left == 0:\n",
        "            self.balance += self.btc_held * current_price\n",
        "            self.btc_held = 0\n",
        "            self._reset_session()\n",
        "        obs, new_price = self._next_observation()\n",
        "        # Rewards given depending on the sign of the stock price\n",
        "        import pdb; pdb.set_trace()\n",
        "        if action == 0:\n",
        "            reward = 1 if new_price == current_price else 0\n",
        "        elif action == 1:\n",
        "            reward = 1 if new_price < current_price else 0\n",
        "        elif action == 2:\n",
        "            reward = 1 if new_price > current_price else 0\n",
        "        done = False\n",
        "        return obs, {}, reward, done, {}\n",
        "\n",
        "    def _get_current_price(self):\n",
        "\n",
        "        return self.df['close'].values[self.current_step:][0]\n",
        "\n",
        "    def _take_action(self, action, current_price):\n",
        "        action_type = action\n",
        "        # amount = action[1] / 10\n",
        "        btc_bought = 0\n",
        "        btc_sold = 0\n",
        "        cost = 0\n",
        "        sales = 0\n",
        "        # if action_type < 1:\n",
        "        #     btc_bought = self.balance / current_price * amount\n",
        "        #     cost = btc_bought * current_price * (1 + self.commission)\n",
        "        #     self.btc_held += btc_bought\n",
        "        #     self.balance -= cost\n",
        "        # elif action_type < 2:\n",
        "        #     btc_sold = self.btc_held * amount\n",
        "        #     sales = btc_sold * current_price * (1 - self.commission)\n",
        "        #     self.btc_held -= btc_sold\n",
        "        #     self.balance += sales\n",
        "\n",
        "        # if btc_sold > 0 or btc_bought > 0:\n",
        "        # self.trades.append({\n",
        "        #   'step': self.frame_start+self.current_step,\n",
        "        #   'amount': btc_sold if btc_sold > 0 else btc_bought,\n",
        "        #   'total': sales if btc_sold > 0 else cost,\n",
        "        #   'type': \"sell\" if btc_sold > 0 else \"buy\"\n",
        "        # })\n",
        "        # self.net_worth = self.balance + self.btc_held * current_price\n",
        "        self.account_history = np.append(self.account_history, [\n",
        "        [self.net_worth],\n",
        "        [btc_bought],\n",
        "        [cost],\n",
        "        [btc_sold],\n",
        "        [sales]\n",
        "        ], axis=1)\n",
        "\n",
        "    # def render(self, mode='human', **kwargs):\n",
        "    #   if mode == 'human':\n",
        "    #     if self.viewer == None:\n",
        "    #       self.viewer = BitcoinTradingGraph(self.df,\n",
        "    #                                         kwargs.get('title', None))\n",
        "    #     self.viewer.render(self.frame_start + self.current_step,\n",
        "    #                        self.net_worth,\n",
        "    #                        self.trades,\n",
        "    #                        window_size=self.lookback_window_size)\n",
        "\n",
        "class EnvInfo():\n",
        "    env_name: str\n",
        "    academy_name: str\n",
        "    num_agents: int\n",
        "    is_discrete: bool\n",
        "    is_visual: bool\n",
        "    is_vector: bool\n",
        "    is_frame_stacking: bool\n",
        "    stack_size: int\n",
        "    shapes: dict\n",
        "    act_size: int\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "env_test = BitcoinTradingEnv(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvaPTbC5J_Tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1de63528-6eb1-4567-a350-836d0bfcf19b"
      },
      "source": [
        "env_test.action_space.n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvCoHdpH5qdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network without envinfo\n",
        "def simple_actor_critic( hidden_sizes= (32, 32), activation= 'relu', activation_output= None, \n",
        "                         kernel_initalizer= 'glorot_uniform', name= 'simple_actor_critic', env_info= EnvInfo):\n",
        "\n",
        "    _actor = mlp(   hidden_sizes= hidden_sizes, output_size= env_info.action_space.n, activation= activation, \n",
        "                    activation_output= activation_output, name= name, kernel_initalizer= kernel_initalizer)\n",
        "    \n",
        "    _critic = mlp(  hidden_sizes= hidden_sizes, output_size= 1, activation= activation, \n",
        "                    activation_output= activation_output, name= name, kernel_initalizer= kernel_initalizer)\n",
        "\n",
        "    log('Model Summary: ' + name)\n",
        "\n",
        "    _actor.build(input_shape = (None,) + env_info.observation_space.shape)\n",
        "    _actor.summary()\n",
        "\n",
        "    _critic.build(input_shape = (None,) + env_info.observation_space.shape)\n",
        "    _critic.summary()\n",
        "\n",
        "    def forward(inp= None):\n",
        "        logits = _actor(inp['vec_obs'])\n",
        "        values = _critic(inp['vec_obs'])\n",
        "        return logits, values\n",
        "\n",
        "    return {\"forward\": forward, \"trainable_networks\": [_actor, _critic]}\n",
        "\n",
        "\n",
        "def vis_vec_actor_critic (  hidden_sizes= (32, 32), activation= 'relu', activation_output= None, \n",
        "                            kernel_initalizer= 'glorot_uniform', name= 'vis_vec_actor_critic', env_info = env_test):\n",
        "\n",
        "    cnn, out_units = cnn_simple()\n",
        "\n",
        "    _mlp_actor = mlp(   hidden_sizes= hidden_sizes, output_size= env_info.action_space, activation= activation, \n",
        "                        activation_output= activation_output, name= name, kernel_initalizer= kernel_initalizer)\n",
        "\n",
        "    _mlp_critic = mlp(  hidden_sizes= hidden_sizes, output_size= 1, activation= activation, \n",
        "                        activation_output= activation_output, name= name, kernel_initalizer= kernel_initalizer)\n",
        "    \n",
        "    log('Model Summary: ' + name)\n",
        "\n",
        "    cnn.build(input_shape = (None,) + env_info.shapes['vis'])\n",
        "    cnn.summary()\n",
        "    _mlp_actor.build(input_shape= (None, env_test.observation_space + out_units))\n",
        "    _mlp_actor.summary()\n",
        "    _mlp_critic.build(input_shape= (None, env_info.observation_space + out_units))\n",
        "    _mlp_critic.summary()\n",
        "\n",
        "    def forward(inp= None):\n",
        "        out_cnn = cnn(inp['vis_obs'])                               # Convolutional Network for visuals \n",
        "        # out_vec_mlp = _mlp_vec(inp['vec_obs])                     # Put vec_obs thorugh Neural Network if to much features \n",
        "        mixed = tf.concat([out_cnn, inp['vec_obs']], -1)            # Concatenate cnn and vec_obs or out_vec_mlp\n",
        "        # out_mixer_mlp = _mlp_mixer(mixed)                         # state mixer with Neural Network if needed\n",
        "        logits = _mlp_actor(mixed)                                  # Feed with raw mixed or with out_mixer_mlp\n",
        "        values = _mlp_critic(mixed)\n",
        "        return logits, values\n",
        "\n",
        "    return {\"forward\": forward, \"trainable_networks\": [cnn, _mlp_actor, _mlp_critic]}\n",
        "\n",
        "\n",
        "class CategoricalModel_2(tf.keras.Model):\n",
        "    \"\"\"\n",
        "        Categorical Model for Discrete Action Spaces\n",
        "        --------------------------------------------\n",
        "            Input:\n",
        "                Network with foward pass and EnvInfo Object\n",
        "            Returns:   \n",
        "                call:                   logits, values from Neural Network \n",
        "                                        with defined forward pass\n",
        "                get-action-lop-value:   logp, action, value \n",
        "                                        (action drawn from Random Categ. Dist)\n",
        "                logp:                   Log probability for action x\n",
        "                entropy:                Entropy Term from logits\n",
        "    \"\"\"\n",
        "    def __init__(self, network= None, env_info= env_test):\n",
        "        super().__init__('CategoricalModel')\n",
        "\n",
        "        self.env_info = env_info\n",
        "        self.act_size = self.env_info.action_space.n                              # Number of possible actio\n",
        "        self.forward = network['forward']                                   # Get feed forward chain\n",
        "        self.all_networks = network['trainable_networks']                   # Get all trainable networks\n",
        "  \n",
        "    def pd(self, logits):\n",
        "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)        # Draw from Random Categorical Distribution\n",
        "\n",
        "    # @tf.function\n",
        "    def call(self, inputs):\n",
        "        return self.forward(inp = inputs) \n",
        "        \n",
        "    def get_action_logp_value(self, obs):\n",
        "        \"\"\"\n",
        "            Returns:\n",
        "            logits --> Last layer of Neural Network without activation function \\n\n",
        "            logp --> SOFTMAX of logits which squashes logits between 0 .. 1 and returns log probabilities \\n\n",
        "            actions --> drawn from normal distribution\n",
        "        \"\"\"\n",
        "        logits, values = self.predict(obs)\n",
        "        import pdb; pdb.set_trace()                                  # Returns np arrays on predict | Input: np array or tensor or list\n",
        "        actions = self.pd(logits)\n",
        "        logp_t = self.logp(logits, actions) \n",
        "        return np.squeeze(actions), np.squeeze(logp_t), np.squeeze(values) \n",
        "\n",
        "    def logp(self, logits, action):\n",
        "        \"\"\"\n",
        "            Returns:\n",
        "            \n",
        "            logp based on the action drawn from prob-distribution \\n\n",
        "            indexes in the logp_all with one_hot\n",
        "        \"\"\"\n",
        "        logp_all = tf.nn.log_softmax(logits)\n",
        "        one_hot = tf.one_hot(action, depth= self.act_size)\n",
        "        logp = tf.reduce_sum( one_hot * logp_all, axis= -1)\n",
        "        return logp\n",
        "        \n",
        "    def entropy(self, logits= None):\n",
        "        \"\"\"\n",
        "            Entropy term for more randomness which means more exploration \\n\n",
        "            Based on OpenAI Baselines implementation\n",
        "        \"\"\"\n",
        "        a0 = logits - tf.reduce_max(logits, axis= -1, keepdims=True)\n",
        "        exp_a0 = tf.exp(a0)\n",
        "        z0 = tf.reduce_sum(exp_a0, axis= -1, keepdims=True)\n",
        "        p0 = exp_a0 / z0\n",
        "        entropy = tf.reduce_sum(p0 * (tf.math.log(z0) - a0), axis= -1)\n",
        "        return entropy\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpPs6XKsQhDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Roller:\n",
        "\n",
        "    def __init__(self, batched_env, model, num_steps, gamma= 0.99, lam= 0.97):\n",
        "\n",
        "        self.batched_env = batched_env\n",
        "        self.batched_env.env_info.is_vector = True\n",
        "        self.batched_env.env_info.is_visual = False\n",
        "        self.model = model\n",
        "        self.num_steps = num_steps\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self._obs_vec = None\n",
        "        self._obs_vis = None\n",
        "        self._rews = None\n",
        "        self._dones = None\n",
        "        self._first_reset = False\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_vec, self._obs_vis, self._rews, self._dones = self.batched_env.reset()\n",
        "\n",
        "    def rollout(self):\n",
        "\n",
        "        if not self._first_reset:                       # self._obs is None: only reset on first time\n",
        "            self.reset()\n",
        "            self._first_reset = True\n",
        "        \n",
        "        vec_obses = []\n",
        "        vis_obses = []\n",
        "        rews = []\n",
        "        dones = []\n",
        "        actions = []\n",
        "        values = []\n",
        "        logp = []\n",
        "        ep_rews = []\n",
        "        ep_lens = []\n",
        "\n",
        "        for step in range(self.num_steps):              # Run each env for num_steps and gather trajectory rollouts\n",
        "\n",
        "            actions_t, logp_t, values_t = self.model.get_action_logp_value({\"vec_obs\": self._obs_vec, \"vis_obs\": self._obs_vis})\n",
        "            import pdb; pdb.set_trace()\n",
        "            vec_obses.append(self._obs_vec) if self.batched_env.env_info.is_vector else None\n",
        "            vis_obses.append(self._obs_vis) if self.batched_env.env_info.is_visual else None\n",
        "            dones.append(self._dones)\n",
        "            actions.append(actions_t)\n",
        "            values.append(values_t)\n",
        "            logp.append(logp_t)\n",
        "\n",
        "            self._obs_vec, self._obs_vis, self._rews, self._dones, infos = self.batched_env.step(actions_t)\n",
        "\n",
        "            for info in infos:\n",
        "                if info is not None:\n",
        "                    ep_rews.append(info['ep_rew'])\n",
        "                    ep_lens.append(info['ep_len'])\n",
        "\n",
        "            rews.append(self._rews)                     # Saves obs(t), dones(t), actions(t), logp(t), values(t) , rews(t+1) in one cycle\n",
        "                                                        # Saves rews(t+1) after env step with action(t)\n",
        "\n",
        "        \"\"\"\n",
        "            End of for loop\n",
        "            ---------------\n",
        "            Get last Values for BOOTSTRAPING\n",
        "        \"\"\"\n",
        "        actions_t, logp_t, values_t = self.model.get_action_logp_value({\"vec_obs\": self._obs_vec, \"vis_obs\": self._obs_vis})\n",
        "        last_values = values_t                          # Bootstraping\n",
        "\n",
        "        vec_obses = np.array(vec_obses, dtype= np.float32) if self.batched_env.env_info.is_vector else None\n",
        "        vis_obses = np.array(vis_obses, dtype= np.float32) if self.batched_env.env_info.is_visual else None\n",
        "        rews = np.array(rews, dtype=np.float32)\n",
        "        dones = np.array(dones, dtype= np.bool)\n",
        "        values = np.array(values, dtype=np.float32)\n",
        "        logp = np.array(logp, dtype=np.float32)\n",
        "        actions = np.array(actions, dtype=np.float32) if isinstance(self.model, GaussianModel) else np.array(actions, dtype=np.int32)\n",
        "        \n",
        "        \"\"\"\n",
        "            Discount / Bootstrap Values and calc Advantages\n",
        "            -----------------------------------------------\n",
        "        \"\"\"\n",
        "        returns = np.zeros_like(rews)\n",
        "        advs = np.zeros_like(rews)\n",
        "        last_gae_lam = 0\n",
        "\n",
        "        for t in reversed(range(self.num_steps)):\n",
        "            if t == self.num_steps - 1:\n",
        "                next_non_terminal = 1.0 - self._dones\n",
        "                next_values = last_values\n",
        "            else:\n",
        "                next_non_terminal = 1.0 - dones[t + 1]\n",
        "                next_values = values[t + 1]\n",
        "            \n",
        "            delta = rews[t] + self.gamma * next_values * next_non_terminal - values[t]\n",
        "            advs[t] = last_gae_lam = delta + self.gamma * self.lam * next_non_terminal * last_gae_lam\n",
        "            \n",
        "        returns = advs + values                         # ADV = RETURNS - VALUES\n",
        "        advs = (advs - advs.mean()) / (advs.std())      # Normalize ADVs\n",
        "\n",
        "        return self._flattened_rollout(vec_obses, vis_obses, rews, dones, actions, logp, values, advs, returns), \\\n",
        "               {'ep_rews': ep_rews, 'ep_lens': ep_lens}\n",
        "    \n",
        "    def _flattened_rollout(self, vec_obses, vis_obses, rews, dones, actions, logp, values, advs, returns):\n",
        "\n",
        "        if self.batched_env.env_info.is_visual:         # Reshape visual obs --> flatten array\n",
        "            d1, d2, d3, d4, d5 = vis_obses.shape\n",
        "            vis_obses = vis_obses.reshape(d1 * d2, d3, d4, d5)\n",
        "        \n",
        "        if self.batched_env.env_info.is_vector:         # Reshape flatten vec obs\n",
        "            vec_obses = vec_obses.reshape(-1, vec_obses.shape[-1])\n",
        "        \n",
        "        if isinstance(self.model, GaussianModel):\n",
        "            actions = actions.reshape(-1, actions.shape[-1])\n",
        "        else:\n",
        "            actions = actions.flatten()\n",
        "\n",
        "        return{     'vec_obses': vec_obses, \n",
        "                    'vis_obses': vis_obses,\n",
        "                    'rews': rews.flatten(), \n",
        "                    'dones': dones.flatten(), \n",
        "                    'actions': actions,\n",
        "                    'logp': logp.flatten(), \n",
        "                    'values': values.flatten(), \n",
        "                    'advs': advs.flatten(), \n",
        "                    'returns': returns.flatten()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apbkhk4kimdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "outputId": "21b38061-37b9-487e-facb-9bc3e410b954"
      },
      "source": [
        "config = '/PPO-Tensorflow-2.0/__WORKING_DIRS__/__STANDARD__/__EXAMPLE__.yaml'\n",
        "\n",
        "params = Params(working_dir= os.getcwd(), config_file= config)          # Get Configuration | HORIZON = Steps per epoch\n",
        "\n",
        "tf.random.set_seed(params.env.seed)                                     # Set Random Seeds for np and tf\n",
        "np.random.seed(params.env.seed)\n",
        "\n",
        "env = BitcoinTradingEnv(data)    # Create Environment in multiprocessing mode\n",
        "LOGGER = Logger('academy_name', os.getcwd(), config)         # Set Logger\n",
        "\n",
        "# network = network_builder(params.trainer.nn_architecure) \\\n",
        "#     (hidden_sizes=params.policy.hidden_sizes, env_info=env.env_info)    # Build Neural Network with Forward Pass\n",
        "\n",
        "network = simple_actor_critic(env_info = env_test)   # Our own version of the simple_actor_critic that fits with the environment\n",
        "\n",
        "model = CategoricalModel_2\n",
        "model = model(network=network, env_info=env_test)                   # Build Model for Discrete or Continuous Spaces\n",
        "\n",
        "if params.trainer.load_model:\n",
        "    log('Loading Model ...')\n",
        "    model.load_weights(LOGGER.tf_model_path('model_weights'))           # Load model if load_model flag set to true\n",
        "\n",
        "roller = Roller(env_test, model, params.trainer.steps_per_epoch,\n",
        "                params.trainer.gamma, params.trainer.lam)               # Define Roller for genrating rollouts for training\n",
        "\n",
        "ppo = PolicyCombinedLoss(model=model, num_envs=1)            # Define PPO Policy with combined loss\n",
        "\n",
        "for epoch in range(params.trainer.epochs):                              # Main training loop for n epochs\n",
        "\n",
        "    rollouts, infos = roller.rollout()                                  # Get Rollout and infos\n",
        "    outs = ppo.update(rollouts)                                         # Push rollout in ppo and update policy accordingly\n",
        "\n",
        "    LOGGER.store('Loss Pi', outs['pi_loss'])\n",
        "    LOGGER.store('Loss V', outs['v_loss'])\n",
        "    LOGGER.store('Loss Ent', outs['entropy_loss'])\n",
        "    LOGGER.store('Appr Ent', outs['approx_ent'])\n",
        "    LOGGER.store('KL DIV', outs['approx_kl'])\n",
        "\n",
        "    for ep_rew, ep_len in zip(infos['ep_rews'], infos['ep_lens']):\n",
        "        LOGGER.store('EP REW', ep_rew)\n",
        "        LOGGER.store('EP LEN', ep_len)\n",
        "\n",
        "    if (epoch % params.trainer.save_freq == 0) or (epoch == params.trainer.epochs - 1):\n",
        "        log('Saving Model ...')\n",
        "        model.save_weights( LOGGER.tf_model_path('model_weights'), \n",
        "                            save_format='tf')                           # Saving model-weights every n steps\n",
        "\n",
        "    LOGGER.log_metrics(epoch)                                           # Push metrics to screen\n",
        "\n",
        "env.close()                                           "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1;36;40m-----------------------------------------------------------------\n",
            "Model Summary: simple_actor_critic\n",
            "-----------------------------------------------------------------\u001b[0m\n",
            "\n",
            "Model: \"simple_actor_critic\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_actor_critic (Dense)  multiple                  64        \n",
            "_________________________________________________________________\n",
            "simple_actor_critic (Dense)  multiple                  1056      \n",
            "_________________________________________________________________\n",
            "simple_actor_critic_output ( multiple                  99        \n",
            "=================================================================\n",
            "Total params: 1,219\n",
            "Trainable params: 1,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"simple_actor_critic\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_actor_critic (Dense)  multiple                  64        \n",
            "_________________________________________________________________\n",
            "simple_actor_critic (Dense)  multiple                  1056      \n",
            "_________________________________________________________________\n",
            "simple_actor_critic_output ( multiple                  33        \n",
            "=================================================================\n",
            "Total params: 1,153\n",
            "Trainable params: 1,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "> <ipython-input-25-31ebf6fd512c>(97)get_action_logp_value()\n",
            "-> actions = self.pd(logits)\n",
            "(Pdb) actions\n",
            "*** NameError: name 'actions' is not defined\n",
            "(Pdb) logits\n",
            "array([[ 4.4288519e+02, -1.1042062e+03, -7.7674615e+02],\n",
            "       [ 4.4640704e+02, -1.1129860e+03, -7.8292249e+02],\n",
            "       [ 4.1845697e+02, -1.0433004e+03, -7.3390277e+02],\n",
            "       [ 4.2667017e+02, -1.0637783e+03, -7.4830756e+02],\n",
            "       [ 2.5310358e+06, -6.3104040e+06, -4.4390100e+06]], dtype=float32)\n",
            "(Pdb) values\n",
            "array([[1.0887468e+03],\n",
            "       [1.0974038e+03],\n",
            "       [1.0286940e+03],\n",
            "       [1.0488853e+03],\n",
            "       [6.2220535e+06]], dtype=float32)\n",
            "(Pdb) simple_actor_critic\n",
            "<function simple_actor_critic at 0x7f9f68d63f28>\n",
            "(Pdb) simple_actor_critic.summary(\n",
            "*** SyntaxError: unexpected EOF while parsing\n",
            "(Pdb) simple_actor_critic.summary()\n",
            "*** AttributeError: 'function' object has no attribute 'summary'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf23K38oQRGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}